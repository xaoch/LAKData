[{"title": "The half-life of MOOC knowledge: a randomized trial evaluating knowledge retention and retrieval practice in MOOCs", "pages": "1-10", "doi": "10.1145/3170358.3170383", "abstract": "Retrieval practice has been established in the learning sciences as one of the most effective strategies to facilitate robust learning in traditional classroom contexts. The cognitive theory underpinning the \"testing effect\" states that actively recalling information is more effective than passively revisiting materials for storing information in long-term memory. We document the design, deployment, and evaluation of an Adaptive Retrieval Practice System (ARPS) in a MOOC. This push-based system leverages the testing effect to promote learner engagement and achievement by intelligently delivering quiz questions from prior course units to learners throughout the course. We conducted an experiment in which learners were randomized to receive ARPS in a MOOC to track their performance and behavior compared to a control group. In contrast to prior literature, we find no significant effect of retrieval practice in this MOOC environment. In the treatment condition, passing learners engaged more with ARPS but exhibited similar levels of knowledge retention as non-passing learners.", "authors": ["Dan Davis", "Ren\u00e9 F. Kizilcec", "Claudia Hauff", "Geert-Jan Houben"]}, {"title": "Graph-based visual topic dependency models: supporting assessment design and delivery at scale", "pages": "11-15", "doi": "10.1145/3170358.3170418", "abstract": "Educational environments continue to rapidly evolve to address the needs of diverse, growing student populations, while embracing advances in pedagogy and technology. In this changing landscape ensuring the consistency among the assessments for different offerings of a course (within or across terms), providing meaningful feedback about students' achievements, and tracking students' progression over time are all challenging tasks, particularly at scale. Here, a collection of visual Topic Dependency Models (TDMs) is proposed to help address these challenges. It visualises the required topics and their dependencies at a course level (e.g., CS 100) and assessment achievement data at the classroom level (e.g., students in CS 100 Term 1 2016 Section 001) both at one point in time (static) and over time (dynamic). The collection of TDMs share a common, two-weighted graph foundation. An algorithm is presented to create a TDM (static achievement for a cohort). An open-source, proof of concept implementation of the TDMs is under development; the current version is described briefly in terms of its support for visualising existing (historical, test) and synthetic data generated on demand.", "authors": ["Kendra Cooper", "Hassan Khosravi"]}, {"title": "Data-driven generation of rubric criteria from an educational programming environment", "pages": "16-20", "doi": "10.1145/3170358.3170399", "abstract": "We demonstrate that, by using a small set of hand-graded student work, we can automatically generate rubric criteria with a high degree of validity, and that a predictive model incorporating these rubric criteria is more accurate than a previously reported model. We present this method as one approach to addressing the often challenging problem of grading assignments in programming environments. A classic solution is creating unit-tests that the student-generated program must pass, but the rigid, structured nature of unit-tests is suboptimal for assessing the more open-ended assignments students encounter in introductory programming environments like Alice. Furthermore, the creation of unit-tests requires predicting the various ways a student might correctly solve a problem - a challenging and time-intensive process. The current study proposes an alternative, semi-automated method for generating rubric criteria using low-level data from the Alice programming environment.", "authors": ["Nicholas Diana", "Michael Eagle", "John Stamper", "Shuchi Grover", "Marie Bienkowski", "Satabdi Basu"]}, {"title": "Supporting teachers' intervention in students' virtual collaboration using a network based model", "pages": "21-25", "doi": "10.1145/3170358.3170394", "abstract": "This paper reports a Design-Based Research project developing a tool (the Process Tab) that supports teachers' interventions with students in virtual internships. The tool uses a networked approach and allows insights into the discourse of groups and individuals based on contributions in chat fora and assignments. In the paper, we present the tool and reports from interviews with three teachers who used the tool. The interviews provide insights about the teachers' hopes, actual use, and difficulties with the tool. The main insight is that even though the teachers genuinely liked the idea and specific representations of the Process Tab, their lack of ability to teach and look at the tool at the same time hindered their use. In the final part of the paper, we discuss how to address this issue.", "authors": ["Tiffany Herder", "Zachari Swiecki", "Simon Skov Fougt", "Andreas Lindenskov Tamborg", "Benjamin Brink Allsopp", "David Williamson Shaffer", "Morten Misfeldt"]}, {"title": "Correlating affect and behavior in reasoning mind with state test achievement", "pages": "26-30", "doi": "10.1145/3170358.3170378", "abstract": "Previous studies have investigated the relationship between affect, behavior, and learning in blended learning systems. These articles have found that affect and behavior are closely linked with learning outcomes. In this paper, we attempt to replicate prior work on how affective states and behaviors relate to mathematics achievement, investigating these issues within the context of 5th-grade students in South Texas using a mathematics blended learning system, Reasoning Mind. We use automatic detectors of student behavior and affect, and correlate inferred rates of each behavior and affective state with the students' end-of-year standardized assessment score. A positive correlation between engaged concentration and test scores replicates previous studies, as does a negative correlation between boredom and test scores. However, our findings differ from previous findings relating to confusion, frustration, and off-task behavior, suggesting the importance of contextual factors for the relationship between behavior, affect, and learning. Our study represents a step in understanding how broadly findings on the relationships between affect/behavior and learning generalize across different learning platforms.", "authors": ["Victor Kostyuk", "Ma. Victoria Almeda", "Ryan S. Baker"]}, {"title": "License to evaluate: preparing learning analytics dashboards for educational practice", "pages": "31-40", "doi": "10.1145/3170358.3170421", "abstract": "Learning analytics can bridge the gap between learning sciences and data analytics, leveraging the expertise of both fields in exploring the vast amount of data generated in online learning environments. A typical learning analytics intervention is the learning dashboard, a visualisation tool built with the purpose of empowering teachers and learners to make informed decisions about the learning process. Related work has investigated learning dashboards, yet none have explored the theoretical foundation that should inform the design and evaluation of such interventions. In this systematic literature review, we analyse the extent to which theories and models from learning sciences have been integrated into the development of learning dashboards aimed at learners. Our analysis revealed that very few dashboard evaluations take into account the educational concepts that were used as a theoretical foundation for their design. Furthermore, we report findings suggesting that comparison with peers, a common reference frame for contextualising information on learning analytics dashboards, was not perceived positively by all learners. We summarise the insights gathered through our literature review in a set of recommendations for the design and evaluation of learning analytics dashboards for learners.", "authors": ["Ioana Jivet", "Maren Scheffel", "Marcus Specht", "Hendrik Drachsler"]}, {"title": "Open learner models and learning analytics dashboards: a systematic review", "pages": "41-50", "doi": "10.1145/3170358.3170409", "abstract": "This paper aims to link student facing Learning Analytics Dashboards (LADs) to the corpus of research on Open Learner Models (OLMs), as both have similar goals. We conducted a systematic review of literature on OLMs and compared the results with a previously conducted review of LADs for learners in terms of (i) data use and modelling, (ii) key publication venues, (iii) authors and articles, (iv) key themes, and (v) system evaluation. We highlight the similarities and differences between the research on LADs and OLMs. Our key contribution is a bridge between these two areas as a foundation for building upon the strengths of each. We report the following key results from the review: in reports of new OLMs, almost 60% are based on a single type of data; 33% use behavioral metrics; 39% support input from the user; 37% have complex models; and just 6% involve multiple applications. Key associated themes include intelligent tutoring systems, learning analytics, and self-regulated learning. Notably, compared with LADs, OLM research is more likely to be interactive (81% of papers compared with 31% for LADs), report evaluations (76% versus 59%), use assessment data (100% versus 37%), provide a comparison standard for students (52% versus 38%), but less likely to use behavioral metrics, or resource use data (33% against 75% for LADs). In OLM work, there was a heightened focus on learner control and access to their own data.", "authors": ["Robert Bodily", "Judy Kay", "Vincent Aleven", "Ioana Jivet", "Dan Davis", "Franceska Xhakaj", "Katrien Verbert"]}, {"title": "Multi-institutional positioning test feedback dashboard for aspiring students: lessons learnt from a case study in flanders", "pages": "51-55", "doi": "10.1145/3170358.3170419", "abstract": "Our work focuses on a multi-institutional implementation and evaluation of a Learning Analytics Dashboards (LAD) at scale, providing feedback to N=337 aspiring STEM (science, technology, engineering and mathematics) students participating in a region-wide positioning test before entering the study program. Study advisors were closely involved in the design and evaluation of the dashboard. The multi-institutional context of our case study requires careful consideration of external stakeholders and data ownership and portability issues, which gives shape to the technical design of the LAD. Our approach confirms students as active agents with data ownership, using an anonymous feedback code to access the LAD and to enable students to share their data with institutions at their discretion. Other distinguishing features of the LAD are the support for active content contribution by study advisors and LATEX type-setting of question item feedback to enhance visual recognizability. We present our lessons learnt from a first iteration in production.", "authors": ["Tom Broos", "Katrien Verbert", "Greet Langie", "Carolien Van Soom", "Tinne De Laet"]}, {"title": "A qualitative evaluation of a learning dashboard to support advisor-student dialogues", "pages": "56-60", "doi": "10.1145/3170358.3170417", "abstract": "This paper presents an evaluation of a learning dashboard that supports the dialogue between a student and a study advisor. The dashboard was designed, developed, and evaluated in collaboration with study advisers. To ensure scalability to other contexts, the dashboard uses data that is commonly available at any higher education institute. It visualizes the grades of the student, an overview of the progress through the year, his/her position in comparison with peers, sliders to plan the next years and a prediction of the length of the bachelor program for this student in years based on historic data. The dashboard was deployed at KU Leuven, Belgium and used in September 2017 to support 224 sessions between students and study advisers. We observed twenty of these conversations. We also collected feedback from 101 students with questionnaires. Results of our observations indicate that the dashboard primarily triggers insights at the beginning of a conversation. The number of insights and the level of these insights (factual, interpretative and reflective) depends on the context of the conversation. Most insights were triggered in conversations with students doubting to continue the program, indicating that our dashboard is useful to support difficult decision-making processes.", "authors": ["Martijn Millecamp", "Francisco Guti\u00e9rrez", "Sven Charleer", "Katrien Verbert", "Tinne De Laet"]}, {"title": "A generalized classifier to identify online learning tool disengagement at scale", "pages": "61-70", "doi": "10.1145/3170358.3170370", "abstract": "Student success, a major focus in higher education, in part, requires students to remain actively engaged in the required coursework. Identifying student disengagement, when a student stops completing coursework, at scale has been a continuing challenge for higher education due to the heterogeneity of traditional college courses. This research uses data from Connect by McGraw-Hill Education, a widely used online learning tool, to build a classifier to identify learning tool disengagement at scale. This classifier was trained and tested on four years of historical data, representing 4.5 million students in 175,000 courses, across 256 disciplines. Results show that the classifier is effective in identifying disengagement within the online learning tool against baselines, across time, and within and across disciplines. The classifier was also effective in identifying students at risk of disengaging from Connect and then earning unsuccessful grades in a pilot course for which the assignments in Connect were worth a relatively small portion of the overall course grade. Because Connect is widely used, this classifier is positioned to be a good tool for instructors and institutions to identify students at risk for disengagement from coursework. Instructors and institutions can use this information to design and implement interventions to improve engagement and improve student success at the institution in key courses.", "authors": ["Jacqueline Feild", "Nicholas Lewkow", "Sean Burns", "Karen Gebhardt"]}, {"title": "Studying MOOC completion at scale using the MOOC replication framework", "pages": "71-78", "doi": "10.1145/3170358.3170369", "abstract": "Research on learner behaviors and course completion within Massive Open Online Courses (MOOCs) has been mostly confined to single courses, making the findings difficult to generalize across different data sets and to assess which contexts and types of courses these findings apply to. This paper reports on the development of the MOOC Replication Framework (MORF), a framework that facilitates the replication of previously published findings across multiple data sets and the seamless integration of new findings as new research is conducted or new hypotheses are generated. In the proof of concept presented here, we use MORF to attempt to replicate 15 previously published findings across 29 iterations of 17 MOOCs. The findings indicate that 12 of the 15 findings replicated significantly across the data sets, and that two findings replicated significantly in the opposite direction. MORF enables larger-scale analysis of MOOC research questions than previously feasible, and enables researchers around the world to conduct analyses on huge multi-MOOC data sets without having to negotiate access to data.", "authors": ["Juan Miguel L. Andres", "Ryan S. Baker", "Dragan Ga\u0161evi\u0107", "George Siemens", "Scott A. Crossley", "Sre\u0107ko Joksimovi\u0107"]}, {"title": "The classroom as a dashboard: co-designing wearable cognitive augmentation for K-12 teachers", "pages": "79-88", "doi": "10.1145/3170358.3170377", "abstract": "When used in classrooms, personalized learning software allows students to work at their own pace, while freeing up the teacher to spend more time working one-on-one with students. Yet such personalized classrooms also pose unique challenges for teachers, who are tasked with monitoring classes working on divergent activities, and prioritizing help-giving in the face of limited time. This paper reports on the co-design, implementation, and evaluation of a wearable classroom orchestration tool for K-12 teachers: mixed-reality smart glasses that augment teachers' realtime perceptions of their students' learning, metacognition, and behavior, while students work with personalized learning software. The main contributions are: (1) the first exploration of the use of smart glasses to support orchestration of personalized classrooms, yielding design findings that may inform future work on real-time orchestration tools; (2) Replay Enactments: a new prototyping method for real-time orchestration tools; and (3) an in-lab evaluation and classroom pilot using a prototype of teacher smart glasses (Lumilo), with early findings suggesting that Lumilo can direct teachers' time to students who may need it most.", "authors": ["Kenneth Holstein", "Gena Hong", "Mera Tegene", "Bruce M. McLaren", "Vincent Aleven"]}, {"title": "An application of participatory action research in advising-focused learning analytics", "pages": "89-96", "doi": "10.1145/3170358.3170387", "abstract": "Advisors assist students in developing successful course pathways through the curriculum. The purpose of this project is to augment advisor institutional and tacit knowledge with knowledge from predictive algorithms (i.e., Matrix Factorization and Classifiers) specifically developed to identify risk. We use a participatory action research approach that directly involves key members from both advising and research communities in the assessment and provisioning of information from the predictive analytics. The knowledge gained from predictive algorithms is evaluated using a mixed method approach. We first compare the predictive evaluations with advisors evaluations of student performance in courses and actual outcomes in those courses We next expose and classify advisor knowledge of student risk and identify ways to enhance the value of the prediction model. The results highlight the contribution that this collaborative approach can give to the constructive integration of Learning Analytics in higher education settings.", "authors": ["Stefano Fiorini", "Adrienne Sewell", "Mathew Bumbalough", "Pallavi Chauhan", "Linda Shepard", "George Rehrey", "Dennis Groth"]}, {"title": "Co-creation strategies for learning analytics", "pages": "97-101", "doi": "10.1145/3170358.3170372", "abstract": "In order to further the field of learning analytics (LA), researchers and experts may need to look beyond themselves and their own perspectives and expertise to innovate LA platforms and interventions. We suggest that by co-creating with the users of LA, such as educators and students, researchers and experts can improve the usability, usefulness, and draw greater understanding from LA interventions. Within this article we discuss the current LA issues and barriers and how co-creation strategies can help address many of these challenges. We further outline the considerations, both pre- and during interventions, which support and foster a co-created strategy for learning analytics interventions.", "authors": ["Mollie Dollinger", "Jason M. Lodge"]}, {"title": "Profiling students from their questions in a blended learning environment", "pages": "102-110", "doi": "10.1145/3170358.3170389", "abstract": "Automatic analysis of learners' questions can be used to improve their level and help teachers in addressing them. We investigated questions (N=6457) asked before the class by 1st year medicine/pharmacy students on an online platform, used by professors to prepare their on-site Q&A session. Our long-term objectives are to help professors in categorizing those questions, and to provide students with feedback on the quality of their questions. To do so, first we manually categorized students' questions, which led to a taxonomy then used for an automatic annotation of the whole corpus. We identified students' characteristics from the typology of questions they asked using K-Means algorithm over four courses. The students were clustered by the proportion of each question asked in each dimension of the taxonomy. Then, we characterized the clusters by attributes not used for clustering such as the students' grade, the attendance, the number and popularity of questions asked. Two similar clusters always appeared: a cluster (A), made of students with grades lower than average, attending less to classes, asking a low number of questions but which are popular; and a cluster (D), made of students with higher grades, high attendance, asking more questions which are less popular. This work demonstrates the validity and the usefulness of our taxonomy, and shows the relevance of this classification to identify different students' profiles.", "authors": ["Fatima Harrak", "Fran\u00e7ois Bouchet", "Vanda Luengo", "Pierre Gillois"]}, {"title": "Recurrence quantification analysis as a method for studying text comprehension dynamics", "pages": "111-120", "doi": "10.1145/3170358.3170407", "abstract": "Self-explanations are commonly used to assess on-line reading comprehension processes. However, traditional methods of analysis ignore important temporal variations in these explanations. This study investigated how dynamical systems theory could be used to reveal linguistic patterns that are predictive of self-explanation quality. High school students (n = 232) generated self-explanations while they read a science text. Recurrence Plots were generated to show qualitative differences in students' linguistic sequences that were later quantified by indices derived by Recurrence Quantification Analysis (RQA). To predict self-explanation quality, RQA indices, along with summative measures (i.e., number of words, mean word length, and type-token ration) and general reading ability, served as predictors in a series of regression models. Regression analyses indicated that recurrence in students' self-explanations significantly predicted human rated self-explanation quality, even after controlling for summative measures of self-explanations, individual differences, and the text that was read (R2 = 0.68). These results demonstrate the utility of RQA in exposing and quantifying temporal structure in student's self-explanations. Further, they imply that dynamical systems methodology can be used to uncover important processes that occur during comprehension.", "authors": ["Aaron D. Likens", "Kathryn S. McCarthy", "Laura K. Allen", "Danielle S. McNamara"]}, {"title": "Towards a writing analytics framework for adult english language learners", "pages": "121-125", "doi": "10.1145/3170358.3170422", "abstract": "Improving the written literacy of newcomers to English-speaking countries can lead to better education, employment, or social integration opportunities. However, this remains a challenge in traditional classrooms where providing frequent, timely, and personalized feedback is not always possible. Analytics can scaffold the writing development of English Language Learners (ELLs) by providing such feedback. To design these analytics, we conducted a field study analyzing essay samples from immigrant adult ELLs (a group often overlooked in writing analytics research) and identifying their epistemic beliefs and learning motivations. We identified common themes across individual learner differences and patterns of errors in the writing samples. The study revealed strong associations between epistemic writing beliefs and learning strategies. The results are used to develop guidelines for designing writing analytics for adult ELLs, and to propose ideas for analytics that scaffold writing development for this group.", "authors": ["Amna Liaqat", "Cosmin Munteanu"]}, {"title": "Epistemic network analysis of students' longer written assignments as formative/summative evaluation", "pages": "126-130", "doi": "10.1145/3170358.3170414", "abstract": "This paper reports on an exploratory trial of developing pedagogical visualizations of 16 students' written assignments on literary analysis using two sets of keywords and Epistemic Network Analysis (ENA). The visualizations are aimed at summative evaluation as a tool for the professor to support assessment and understanding of subject learning. Results show that ENA can visually distinguish low, middle and high performing students, but not statistically significantly. Thus, our trial provides a tool for the professor that supports understanding of subject learning and formative assessment.", "authors": ["Simon Skov Fougt", "Amanda Siebert-Evenstone", "Brendan Eagan", "Sara Tabatabai", "Morten Misfeldt"]}, {"title": "Driving data storytelling from learning design", "pages": "131-140", "doi": "10.1145/3170358.3170380", "abstract": "Data science is now impacting the education sector, with a growing number of commercial products and research prototypes providing learning dashboards. From a human-centred computing perspective, the end-user's interpretation of these visualisations is a critical challenge to design for, with empirical evidence already showing that `usable' visualisations are not necessarily effective from a learning perspective. Since an educator's interpretation of visualised data is essentially the construction of a narrative about student progress, we draw on the growing body of work on Data Storytelling (DS) as the inspiration for a set of enhancements that could be applied to data visualisations to improve their communicative power. We present a pilot study that explores the effectiveness of these DS elements based on educators' responses to paper prototypes. The dual purpose is understanding the contribution of each visual element for data storytelling, and the effectiveness of the enhancements when combined.", "authors": ["Vanessa Echeverria", "Roberto Martinez-Maldonado", "Roger Granda", "Katherine Chiluiza", "Cristina Conati", "Simon Buckingham Shum"]}, {"title": "Linking students' timing of engagement to learning design and academic performance", "pages": "141-150", "doi": "10.1145/3170358.3170398", "abstract": "In recent years, the connection between Learning Design (LD) and Learning Analytics (LA) has been emphasized by many scholars as it could enhance our interpretation of LA findings and translate them to meaningful interventions. Together with numerous conceptual studies, a gradual accumulation of empirical evidence has indicated a strong connection between how instructors design for learning and student behaviour. Nonetheless, students' timing of engagement and its relation to LD and academic performance have received limited attention. Therefore, this study investigates to what extent students' timing of engagement aligned with instructor learning design, and how engagement varied across different levels of performance. The analysis was conducted over 28 weeks using trace data, on 387 students, and replicated over two semesters in 2015 and 2016. Our findings revealed a mismatch between how instructors designed for learning and how students studied in reality. In most weeks, students spent less time studying the assigned materials on the VLE compared to the number of hours recommended by instructors. The timing of engagement also varied, from in advance to catching up patterns. High-performing students spent more time studying in advance, while low-performing students spent a higher proportion of their time on catching-up activities. This study reinforced the importance of pedagogical context to transform analytics into actionable insights.", "authors": ["Quan Nguyen", "Michal Huptych", "Bart Rienties"]}, {"title": "Video and learning: a systematic review (2007--2017)", "pages": "151-160", "doi": "10.1145/3170358.3170376", "abstract": "Video materials have become an integral part of university learning and teaching practice. While empirical research concerning the use of videos for educational purposes has increased, the literature lacks an overview of the specific effects of videos on diverse learning outcomes. To address such a gap, this paper presents preliminary results of a large-scale systematic review of peer-reviewed empirical studies published from 2007-2017. The study synthesizes the trends observed through the analysis of 178 papers selected from the screening of 2531 abstracts. The findings summarize the effects of manipulating video presentation, content and tasks on learning outcomes, such as recall, transfer, academic achievement, among others. The study points out the gap between large-scale analysis of fine-grained data on video interaction and experimental findings reliant on established psychological instruments. Narrowing this gap is suggested as the future direction for the research on video-based learning.", "authors": ["Oleksandra Poquet", "Lisa Lim", "Negin Mirriahi", "Shane Dawson"]}, {"title": "Using embedded formative assessment to predict state summative test scores", "pages": "161-170", "doi": "10.1145/3170358.3170392", "abstract": "If we wish to embed assessment for accountability within instruction, we need to better understand the relative contribution of different types of learner data to statistical models that predict scores on assessments used for accountability purposes. The present work scales up and extends predictive models of math test scores from existing literature and specifies six categories of models that incorporate information about student prior knowledge, socio-demographics, and performance within the MATHia intelligent tutoring system. Linear regression and random forest models are learned within each category and generalized over a sample of 23,000+ learners in Grades 6, 7, and 8 over three academic years in Miami-Dade County Public Schools. After briefly exploring hierarchical models of this data, we discuss a variety of technical and practical applications, limitations, and open questions related to this work, especially concerning to the potential use of instructional platforms like MATHia as a replacement for time-consuming standardized tests.", "authors": ["Stephen E. Fancsali", "Guoguo Zheng", "Yanyan Tan", "Steven Ritter", "Susan R. Berman", "April Galyardt"]}, {"title": "The influence of students' cognitive and motivational characteristics on students' use of a 4C/ID-based online learning environment and their learning gain", "pages": "171-180", "doi": "10.1145/3170358.3170363", "abstract": "Research has revealed that the design of online learning environments can influence students' use and performance. In this study, an online learning environment for learning French as a foreign language was developed in line with the four component instructional design (4C/ID) model. While the 4C/ID-model is a well-established instructional design model, little is known about (1) factors impacting students' use of the four components, namely, learning tasks, part-task practice, supportive and procedural information during their learning process as well as about (2) the way in which students' differences in use of the 4C/ID-based online learning environment impacts course performance. The aim of this study is, therefore, twofold. Firstly, it investigates the influence of students' prior knowledge, task value and self-efficacy on students' use of the four different components of the 4C/ID-model. Secondly, it examines the influence of students' use of the components on their learning gain, taking into account their characteristics. The sample consisted of 161 students in higher education. Results, based on structural equation modelling (SEM), indicate that prior knowledge has a negative influence on students' use of learning tasks and part-task practice. Task value has a positive influence on use of learning tasks and supportive information. Additionally, results indicate that use of use of learning tasks, procedural information, controlled for students' prior knowledge significantly contribute to students' learning gain. Results suggest that students' use of the four components is based on their cognitive and motivational characteristics. Furthermore, results reveal the impact of students' use of learning tasks and procedural information on students' learning gain.", "authors": ["Charlotte Larmuseau", "Jan Elen", "Fien Depaepe"]}, {"title": "Explaining learning performance using response-time, self-regulation and satisfaction from content: an fsQCA approach", "pages": "181-190", "doi": "10.1145/3170358.3170397", "abstract": "This study focuses on compiling students' response-time allocated to answer correctly or wrongly, their self-regulation, as well as their satisfaction from content, in order to explain high or medium/low learning performance. To this end, it proposes a conceptual model in conjunction with research propositions. For the evaluation of the approach, an empirical study with 452 students was conducted. The fuzzy set qualitative comparative analysis (fsQCA) revealed five configurations driven by the admitted factors that explain students' high performance, as well as five additional patterns, interpreting students' medium/low performance. These findings advance our understanding of the relations between actual usage and latent behavioral factors, as well as their combined effect on students' test score. Limitations and potential implications of these findings are also discussed.", "authors": ["Zacharoula Papamitsiou", "Anastasios A. Economides", "Ilias O. Pappas", "Michail N. Giannakos"]}, {"title": "Finding traces of self-regulated learning in activity streams", "pages": "191-200", "doi": "10.1145/3170358.3170381", "abstract": "This paper aims to identify self-regulation strategies from students' interactions with the learning management system (LMS). We used learning analytics techniques to identify metacognitive and cognitive strategies in the data. We define three research questions that guide our studies analyzing i) self-assessments of motivation and self regulation strategies using standard methods to draw a baseline, ii) interactions with the LMS to find traces of self regulation in observable indicators, and iii) self regulation behaviours over the course duration. The results show that the observable indicators can better explain self-regulatory behaviour and its influence in performance than preliminary subjective assessments.", "authors": ["Anal\u00eda Cicchinelli", "Eduardo Veas", "Abelardo Pardo", "Viktoria Pammer-Schindler", "Angela Fessl", "Carla Barreiros", "Stefanie Lindst\u00e4dt"]}, {"title": "Investigating learning strategies in a dispositional learning analytics context: the case of worked examples", "pages": "201-205", "doi": "10.1145/3170358.3170385", "abstract": "This study aims to contribute to recent developments in empirical studies on students' learning strategies, whereby the use of trace data is combined with self-report data to distinguish profiles of learning strategy use [3--5]. We do so in the context of an application of dispositional learning analytics in a large introductory course mathematics and statistics, based on blended learning. Building on our previous work which showed marked differences in how students used worked examples as a learning strategy [7, 11], this study compares different profiles of learning strategies with learning approaches, learning outcomes, and learning dispositions. One of our key findings is that deep learners were less dependent on worked examples as a resource for learning, and that students who only sporadically used worked examples achieved higher test scores.", "authors": ["Dirk Tempelaar", "Bart Rienties", "Quan Nguyen"]}, {"title": "Discovery and temporal analysis of latent study patterns in MOOC interaction sequences", "pages": "206-215", "doi": "10.1145/3170358.3170388", "abstract": "Capturing students' behavioral patterns through analysis of sequential interaction logs is an important task in educational data mining and could enable more effective and personalized support during the learning processes. This study aims at discovery and temporal analysis of learners' study patterns in MOOC assessment periods. We propose two different methods to achieve this goal. First, following a hypothesis-driven approach, we identify learners' study patterns based on their interaction with lectures and assignments. Through clustering of study pattern sequences, we capture different longitudinal activity profiles among learners and describe their properties. Second, we propose a temporal clustering pipeline for unsupervised discovery of latent patterns in learners' interaction data. We model and cluster activity sequences at each time step and perform cluster matching to enable tracking learning behaviours over time. Our proposed pipeline is general and applicable in different learning environments such as MOOC and ITS. Moreover, it allows for modeling and temporal analysis of interaction data at different levels of actions granularity and time resolution. We demonstrate the application of this method for detecting latent study patterns in a MOOC course.", "authors": ["Mina Shirvani Boroujeni", "Pierre Dillenbourg"]}, {"title": "Evaluating retrieval practice in a MOOC: how writing and reading summaries of videos affects student learning", "pages": "216-225", "doi": "10.1145/3170358.3170382", "abstract": "Videos are often the core content in open online education, such as in Massive Open Online Courses (MOOCs). Students spend most of their time in a MOOC on watching educational videos. However, merely watching a video is a relatively passive learning activity. To increase the educational benefits of online videos, students could benefit from more actively interacting with the to-be-learned material. In this paper two studies (n = 13k) are presented which examined the educational benefits of two more active learning strategies: 1) Retrieval Practice tasks which asked students to shortly summarize the content of videos, and 2) Given Summary tasks in which the students were asked to read pre-written summaries of videos. Writing, as well as reading summaries of videos were positively related to quiz grades. Both interventions seemed to help students to perform better, but there was no apparent difference between the efficacy of these interventions. These studies show how the quality of online education can be improved by adapting course design to established approaches from the learning sciences.", "authors": ["Tim van der Zee", "Dan Davis", "Nadira Saab", "Bas Giesbers", "Jasper Ginn", "Frans van der Sluis", "Fred Paas", "Wilfried Admiraal"]}, {"title": "Reciprocal peer recommendation for learning purposes", "pages": "226-235", "doi": "10.1145/3170358.3170400", "abstract": "Larger student intakes by universities and the rise of education through Massive Open Online Courses has led to less direct contact time with teaching staff for each student. One potential way of addressing this contact deficit is to invite learners to engage in peer learning and peer support; however, without technological support they may be unable to discover suitable peer connections that can enhance their learning experience. Two different research subfields with ties to recommender systems provide partial solutions to this problem. Reciprocal recommender systems provide sophisticated filtering techniques that enable users to connect with one another. To date, however, the main focus of reciprocal recommender systems has been on providing recommendation in online dating sites. Recommender systems for technology enhanced learning have employed and tailored exemplary recommenders towards use in education, with a focus on recommending learning content rather than other users. In this paper, we first discuss the importance of supporting peer learning and the role recommending reciprocal peers can play in educational settings. We then introduce our open-source course-level recommendation platform called RiPPLE that has the capacity to provide reciprocal peer recommendation. The proposed reciprocal peer recommender algorithm is evaluated against key criteria such as scalability, reciprocality, coverage, and quality and shows improvement over a baseline recommender. Primary results indicate that the system can help learners connect with peers based on their knowledge gaps and reciprocal preferences, with designed flexibility to address key limitations of existing algorithms identified in the literature.", "authors": ["Boyd A. Potts", "Hassan Khosravi", "Carl Reidsema", "Aneesha Bakharia", "Mark Belonogoff", "Melanie Fleming"]}, {"title": "Rethinking learning analytics adoption through complexity leadership theory", "pages": "236-244", "doi": "10.1145/3170358.3170375", "abstract": "Despite strong interest in learning analytics (LA), adoption at a large-scale organizational level continues to be problematic. This may in part be due to the lack of acknowledgement of existing conceptual LA models to operationalize how key adoption dimensions interact to inform the realities of the implementation process. This paper proposes the framing of LA adoption in complexity leadership theory (CLT) to study the overarching system dynamics. The framing is empirically validated in a study analysing interviews with senior staff in Australian universities (n=32). The results were coded for several adoption dimensions including leadership, governance, staff development, and culture. The coded data were then analysed with latent class analysis. The results identified two classes of universities that either i) followed an instrumental approach to adoption - typically top-down leadership, large scale project with high technology focus yet demonstrating limited staff uptake; or ii) were characterized as emergent innovators - bottom up, strong consultation process, but with subsequent challenges in communicating and scaling up innovations. The results suggest there is a need to broaden the focus of research in LA adoption models to move on from small-scale course/program levels to a more holistic and complex organizational level.", "authors": ["Shane Dawson", "Oleksandra Poquet", "Cassandra Colvin", "Tim Rogers", "Abelardo Pardo", "Dragan Gasevic"]}, {"title": "Capitalisation of analysis processes: enabling reproducibility, openness and adaptability thanks to narration", "pages": "245-254", "doi": "10.1145/3170358.3170408", "abstract": "Analysis processes of learning traces, used to gain important pedagogical insights, are yet to be easily shared and reused. They face what is commonly called a reproducibility crisis. From our observations, we identify two important factors that may be the cause of this crisis: technical constraints due to runnable necessities, and context dependencies. Moreover, the meaning of the reproducibility itself is ambiguous and a source of misunderstanding. In this paper, we present an ontological framework dedicated to taking full advantage of already implemented educational analyses. This framework shifts the actual paradigm of analysis processes by representing them from a narrative point of view, instead of a technical one. This enables a formal description of analysis processes with high-level concepts. We show how this description is performed, and how it can help analysts. The goal is to empower both expert and non-expert analysis stakeholders with the possibility to be involved in the elaboration of analysis processes and their reuse in different contexts, by improving both human and machine understanding of these analyses. This possibility is known as the capitalisation of analysis processes of learning traces.", "authors": ["Alexis Lebis", "Marie Lefevre", "Vanda Luengo", "Nathalie Guin"]}, {"title": "Classroom size, activity and attendance: scaling up drivers of learning space occupation", "pages": "255-259", "doi": "10.1145/3170358.3170401", "abstract": "Teaching face-to-face is still a major education mode in many universities, yet institutions are increasingly tasked with improving efficient use of teaching spaces. This need to understand space use can be coupled with learning and teaching data to better inform student attendance and subsequently participation. Here, we analyse thermal sensor data used to monitor traffic into classrooms; these data are associated with the timetable to provide knowledge of the course and the teaching mode (such as lecture, tutorial or workshop). Further, we integrate these traffic data with student feedback data to investigate the drivers of student attendance patterns, and aim to also include online activity and behaviour to develop broad models of both room occupancy and student attendance. Combining space utilisation data with information on teaching modality and in-class and out-of-class participation can inform on how to both improve learning and design effective and efficient teaching spaces.", "authors": ["Amelia Brennan", "Christina Peace", "Pablo Munguia"]}, {"title": "Towards a data archiving solution for learning analytics", "pages": "260-264", "doi": "10.1145/3170358.3170415", "abstract": "Data solutions in the teaching and learning space are in need of pro-active innovations in data management, to ensure that systems for learning analytics can scale up to match the size of datasets now available. Here, we illustrate the scale at which a Learning Management System (LMS) accumulates data, and discuss the barriers to using this data for in-depth analyses. We illustrate the exponential growth of our LMS data to represent a single example dataset, and highlight the broader need for taking a pro-active approach to dimensional modelling in learning analytics, anticipating that common learning analytics questions will be computationally expensive, and that the most useful data structures for learning analytics will not necessarily follow those of the source dataset.", "authors": ["Sarah Taylor", "Pablo Munguia"]}, {"title": "Connecting decentralized learning records: a blockchain based learning analytics platform", "pages": "265-269", "doi": "10.1145/3170358.3170365", "abstract": "As Learners move from one learning environment to another, there is a key necessity of taking with them a proof of previous learning achievements or experiences. In most cases, this is either expressed in terms of receipt of scores or a certificate of completion. While this may be sufficient for enrollment and other administrative decisions, it poses some limitations to the depth of learning analytics and consequently a slow onboarding process. Also, with different institutions having their learning data isolated from each other, it becomes more difficult to easily access a learner's learning history for all learning activities on other systems. In this paper, we propose a blockchain based approach for connecting learning data across different Learning Management Systems (LMS), Learning Record Stores (LRS), institutions and organizations. Leveraging on unique properties of blockchain technology, we also propose solutions to ensuring learning data consistency, availability, immutability, security, privacy and access control.", "authors": ["Patrick Ocheja", "Brendan Flanagan", "Hiroaki Ogata"]}, {"title": "Running out of STEM: a comparative study across STEM majors of college students at-risk of dropping out early", "pages": "270-279", "doi": "10.1145/3170358.3170410", "abstract": "Higher education institutions in the United States and across the Western world face a critical problem of attrition of college students and this problem is particularly acute within the Science, Technology, Engineering, and Mathematics (STEM) fields. Students are especially vulnerable in the initial years of their academic programs; more than 60% of the dropouts occur in the first two years. Therefore, early identification of at-risk students is crucial for a focused intervention if institutions are to support students towards completion. In this paper we developed and evaluated a survival analysis framework for the early identification of students at the risk of dropping out. We compared the performance of survival analysis approaches to other machine learning approaches including logistic regression, decision trees and boosting. The proposed methods show good performance for early prediction of at-risk students and are also able to predict when a student will dropout with high accuracy. We performed a comparative analysis of nine different majors with varying levels of academic rigor, challenge and student body. This study enables advisors and university administrators to intervene in advance to improve student retention.", "authors": ["Yujing Chen", "Aditya Johri", "Huzefa Rangwala"]}, {"title": "Prospectively predicting 4-year college graduation from student applications", "pages": "280-289", "doi": "10.1145/3170358.3170395", "abstract": "We leverage a unique national dataset of 41,359 college applications to prospectively predict 4-year bachelor's graduation in a generalizable manner. Our features include sociodemographics, institutional graduation rates, academic achievement, standardized test scores, engagement in extracurricular activities, work experiences, and ratings by teachers and high-school guidance counselors. A random forest classifier successfully predicted 4-year graduation for 71.4% of the students (base rate = 44%) using all 166 of the aforementioned features and a split-half validation method. A stochastic hill-climbing feature selection procedure effectively maintained the same classification accuracy, but with a minimal set of 37 features, consisting of an approximately equal representation of sociodemographics, cognitive, and noncognitive factors. We advocate against using these results for admissions decisions, instead contemplating how they might be used to provide parents and educators with actionable information to guide students towards college success.", "authors": ["Stephen Hutt", "Margo Gardener", "Donald Kamentz", "Angela L. Duckworth", "Sidney K. D'Mello"]}, {"title": "\"I'll do it!\": examining the relationship between locus of control and math game retention for preschoolers", "pages": "290-294", "doi": "10.1145/3170358.3170368", "abstract": "Acquiring simple arithmetic skills at the preschool level requires repetitive practices. One method for encouraging students to spend longer time practicing is by presenting the skills in an engaging game. As student retention on the game increases, the student will be more likely to acquire the practiced skill since she will have spent more time practicing. In this paper, we examine the relationship between internal locus of control and retention in game-based learning applications for young children using Todo Math, a mobile-based math learning application for children from Pre-K to 2nd grade. We examine 345,783 users' log data to show that when children prefer \"free\" mode, which has high internal locus of control, their retention on Todo Math is higher than children who prefer \"daily\" mode, which has high external locus of control. We present three analyses that support our findings using survival analysis, post-hoc analysis, and t-test.", "authors": ["Bugeun Kim", "Jungwook Rhim", "Jihyun Rho", "Taehyun Hwang", "Gunho Lee", "Gahgene Gweon"]}, {"title": "Coenrollment networks and their relationship to grades in undergraduate education", "pages": "295-304", "doi": "10.1145/3170358.3170373", "abstract": "In this paper, we evaluate the complete undergraduate coenrollment network over a decade of education at a large American public university. We provide descriptive properties of the network, demonstrating that the coenrollment networks evaluated follow power-law degree distributions similar to many other large-scale networks; that they reveal strong performance-based assortativity; and that network-based features can significantly improve GPA-based student performance predictors. We then implement a network-based, multi-view classification model to predict students' final course grades. In particular, we adapt a structural modeling approach from [19, 34], whereby we model the university-wide undergraduate coenrollment network as an undirected graph. We compare the performance of our predictor to traditional methods used for grade prediction in undergraduate university courses, and demonstrate that a multi-view ensembling approach outperforms both prior \"flat\" and network-based models for grade prediction across several classification metrics. These findings demonstrate the usefulness of combining diverse approaches in models of student success, and demonstrate specific network-based modeling strategies which are likely to be most effective for grade prediction.", "authors": ["Josh Gardner", "Christopher Brooks"]}, {"title": "Conceptualizing co-enrollment: accounting for student experiences across the curriculum", "pages": "305-309", "doi": "10.1145/3170358.3170366", "abstract": "In this study, we develop and test three measures for conceptualizing the potential impact of co-enrollment in different courses on students' changing risk for academic difficulty in a focal course. Two of these measures, concurrent enrollment in at least one difficult course and academic difficulty in the prior week in courses other than the focal course, significantly increase students' odds of academic difficulty in the focal course in our models. Our results have implications for the designs of Early Warning Systems and suggest that academic planners consider the relationship between course co-enrollment and students' academic success.", "authors": ["Michael Geoffrey Brown", "R. Matthew DeMonbrun", "Stephanie D. Teasley"]}, {"title": "A framework for developing metrics of youth engagement in informal learning environments", "pages": "310-314", "doi": "10.1145/3170358.3170393", "abstract": "This paper proposes a framework which aims to leverage data from informal learning environments to provide insights about youth engagement for various stakeholders. To explore the framework, we created metrics to examine the engagement of 98 middle school-aged girls during a 20-week STEM program using attendance records and log data from an online learning platform coded to reflect 21st century learning activities. We present preliminary analyses using the metrics, focusing on how they can help stakeholders understand engagement and equity of participation.", "authors": ["Denise Nacu", "Jennifer Baltes", "Taha Hamid", "Jonathan Gemmell", "Nichole Pinkard"]}, {"title": "A diagnostic tool for competency-based program engineering", "pages": "315-319", "doi": "10.1145/3170358.3170402", "abstract": "Competency based education (CBE) is seen by many as a way to optimize learning on cost, efficiency and flexibility. However, defining the required competencies, assigning them to specific courses and building the assessments evaluating student's proficiency can be tedious. More precisely, making sure that the assessments evaluate what they are supposed to evaluate requires a fair amount of psychometrics knowledge and time that can be difficult for teachers to acquire, maintain and use. Addressing assessment validity and more specifically competency frameworks mapping adequacy, we propose a rule-based tool to ease the building and the refinement of CBE courses and curricula. After introducing the context and briefly the related work, we present our set of rules before illustrating the capacity of the proposed diagnostic tool on an engineering curriculum. Experiments show that this tool can improve mapping adequacy in term of predictive accuracy and would require more efforts towards competency parameters reliability measurement.", "authors": ["Guillaume Durand", "Cyril Goutte", "Nabil Belacel", "Yassine Bouslimani", "Serge L\u00e9ger"]}, {"title": "SHEILA policy framework: informing institutional strategies and policy processes of learning analytics", "pages": "320-329", "doi": "10.1145/3170358.3170367", "abstract": "This paper introduces a learning analytics policy development framework developed by a cross-European research project team - SHEILA (Supporting Higher Education to Integrate Learning Analytics), based on interviews with 78 senior managers from 51 European higher education institutions across 16 countries. The framework was developed using the RAPID Outcome Mapping Approach (ROMA), which is designed to develop effective strategies and evidence-based policy in complex environments. This paper presents three case studies to illustrate the development process of the SHEILA policy framework, which can be used to inform strategic planning and policy processes in real world environments, particularly for large-scale implementation in higher education contexts.", "authors": ["Yi-Shan Tsai", "Pedro Manuel Moreno-Marcos", "Kairit Tammets", "Kaire Kollom", "Dragan Ga\u0161evi\u0107"]}, {"title": "Unpacking the relationship between discussion forum participation and learning in MOOCs: content is key", "pages": "330-339", "doi": "10.1145/3170358.3170403", "abstract": "This study examined the relationship between discussion forum contributions and course assessment results in a statistics MOOC. An important feature of the study is that it distinguished between discussions that were related to the learning of course material (\"content-related\") and those which were not (\"non-content\"). Another contribution is that the study evaluated the additional usefulness of social centrality measures in predicting course grade after the quantity of forum contributions has been accounted for. Results showed that, overall, 15% of course learners contributed to the forums and these learners had a significantly higher rate of successfully passing the course than non-contributors (64% vs 32% passing). Learners who made posts to both content-related and non-content threads had a higher passing rate than those who only contributed to one type or the other. Among learners who successfully passed the course, there were no differences in course grade when comparing discussion contributors and non-contributors overall; however those who contributed to content-related threads performed slightly better than those who did not (course grade of 87% vs 85%). A predictive model based on the number of posts made to content-related threads explained a small proportion of variance in course grades; addition of social centrality measures did not significantly improve the variance explained by the model.", "authors": ["Alyssa Friend Wise", "Yi Cui"]}, {"title": "Are MOOC forums changing?", "pages": "340-349", "doi": "10.1145/3170358.3170416", "abstract": "There has been a growing trend in higher education towards increased use and adoption of Massive Open Online Courses (MOOCs). Despite this interest in learning at scale, limited work has compared MOOC activity across subsequent course offerings. In this study, we explore forum activity in ten iterations of the same MOOC. Our results suggest that participation in MOOC forums has changed over the past four years of delivery. First, overall participation in MOOC forums have decreased. Second, in later iterations cohorts of more committed forum users start to resemble formal online courses in size (67>n>36). However, despite the smaller groups of learners that should find it easier to form connections with one another, our analysis did not reveal the expected increase in the quality of social activity. Instead, MOOC forums evolved into smaller on-task question and answer (Q&A) spaces, not capitalizing on the opportunities for social learning. We discuss practical and research implications of such changes.", "authors": ["Oleksandra Poquet", "Nia Dowell", "Christopher Brooks", "Shane Dawson"]}, {"title": "Gaze insights into debugging behavior using learner-centred analysis", "pages": "350-359", "doi": "10.1145/3170358.3170386", "abstract": "The presented study tries to tackle an intriguing question of how user-generated data from current technologies can be used to reinforce learners' reflections, improve teaching practices, and close the learning analytics loop. In particular, the aim of the study is to utilize users' gaze to examine the role of a mirroring tool (i.e. Exercise View in Eclipse) in orchestrating basic behavioral regulation of participants engaged in a debugging task. The results demonstrated that students who processed the information presented in the Exercise View and acted upon it, improved their performance and achieved higher level of success than those who failed to do it. The findings shed a light how to capture what constitute relevant data within a particular context using gaze patterns, that could guide collection of essential learner-centred analytics for the purpose of designing usable and modular learning environments based on data-driven approaches.", "authors": ["Katerina Mangaroska", "Kshitij Sharma", "Michail Giannakos", "Hallvard Tr\u00e6tteberg", "Pierre Dillenbourg"]}, {"title": "The RAP system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors", "pages": "360-364", "doi": "10.1145/3170358.3170406", "abstract": "Developing communication skills in higher education students could be a challenge to professors due to the time needed to provide formative feedback. This work presents RAP, a scalable system to provide automatic feedback to entry-level students to develop basic oral presentation skills. The system improves the state-of-the-art by analyzing posture, gaze, volume, filled pauses and the slides of the presenters through data captured by very low-cost sensors. The system also provides an off-line feedback report with multimodal recordings of their performance. An initial evaluation of the system indicates that the system's feedback highly agrees with human feedback and that students considered that feedback useful to develop their oral presentation skills.", "authors": ["Xavier Ochoa", "Federico Dom\u00ednguez", "Bruno Guam\u00e1n", "Ricardo Maya", "Gabriel Falcones", "Jaime Castells"]}, {"title": "(Dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics", "pages": "365-369", "doi": "10.1145/3170358.3170420", "abstract": "Video analysis is a staple of the education research community. For many contemporary education researchers, participation in the video coding process serves as a rite of passage. However, recent developments in multimodal learning analytics may help to accelerate and enhance this process by providing researchers with a more nuanced glimpse into a set of learning experiences. As an example of how to use multimodal learning analytics towards these ends, this paper includes a preliminary analysis from 54 college students, who completed two engineering design tasks in pairs. Gesture, speech and electro-dermal activation data were collected as students completed these tasks. The gesture data was used to learn a set of canonical clusters (N=4). A decision tree was trained based on individual students' cluster frequencies, and pre-post learning gains. The nodes in the decision tree were then used to identify a subset of video segments that were human coded based on prior work in learning analytics and engineering design. The combination of machine learning and human inference helps elucidate the practices that seem to correlate with student learning. In particular, both engagement and disengagement seem to correlate with student learning, albeit in a somewhat nuanced fashion.", "authors": ["Marcelo Worsley"]}, {"title": "Analysis of interactions between lecturers and students", "pages": "370-374", "doi": "10.1145/3170358.3170360", "abstract": "In this paper, we discuss the interactions between lecturers and students. First, we detect their behaviors by extracting the face regions of lecturers and students from moving images. Thereafter, we model the interaction between the lecturers' writing and explanation behaviors and the students' note taking and listening behaviors by using multilayered neural networks. We discuss the above interactions based on the internal representations of multilayered neural networks.", "authors": ["Eiji Watanabe", "Takashi Ozeki", "Takeshi Kohama"]}, {"title": "Physical learning analytics: a multimodal perspective", "pages": "375-379", "doi": "10.1145/3170358.3170379", "abstract": "The increasing progress in ubiquitous technology makes it easier and cheaper to track students' physical actions unobtrusively, making it possible to consider such data for supporting research, educator interventions, and provision of feedback to students. In this paper, we reflect on the underexplored, yet important area of learning analytics applied to physical/motor learning tasks and to the physicality aspects of `traditional' intellectual tasks that often occur in physical learning spaces. Based on Distributed Cognition theory, the concept of Internet of Things and multimodal learning analytics, this paper introduces a theoretical perspective for bringing learning analytics into physical spaces. We present three prototypes that serve to illustrate the potential of physical analytics for teaching and learning. These studies illustrate advances in proximity, motion and location analytics in collaborative learning, dance education and healthcare training.", "authors": ["Roberto Martinez-Maldonado", "Vanessa Echeverria", "Olga C. Santos", "Augusto Dias Pereira Dos Santos", "Kalina Yacef"]}, {"title": "A multi-dimensional analysis of writing flexibility in an automated writing evaluation system", "pages": "380-388", "doi": "10.1145/3170358.3170404", "abstract": "The assessment of writing proficiency generally includes analyses of the specific linguistic and rhetorical features contained in the singular essays produced by students. However, researchers have recently proposed that an individual's ability to flexibly adapt the linguistic properties of their writing might more closely capture writing skill. However, the features of the task, learner, and educational context that influence this flexibility remain largely unknown. The current study extends this research by examining relations between linguistic flexibility, reading comprehension ability, and feedback in the context of an automated writing evaluation system. Students (n = 131) wrote and revised six essays in an automated writing evaluation system and were provided both summative and formative feedback on their writing. Additionally, half of the students had access to a spelling and grammar checker that provided lower-level feedback during the writing period. The results provide evidence for the fact that developing writers demonstrate linguistic flexibility across the essays that they produce. However, analyses also indicate that lower-level feedback (i.e., spelling and grammar feedback) have little to no impact on the properties of students' essays nor on their variability across prompts or drafts. Overall, the current study provides important insights into the role of flexibility in writing skill and develops a strong foundation on which to conduct future research and educational interventions.", "authors": ["Laura K. Allen", "Aaron D. Likens", "Danielle S. McNamara"]}, {"title": "Understand students' self-reflections through learning analytics", "pages": "389-398", "doi": "10.1145/3170358.3170374", "abstract": "Reflective writing has been widely recognized as one of the most effective activities for fostering students' reflective and critical thinking. The analysis of students' reflective writings has been the focus of many research studies. However, to date this has been typically a very labor-intensive manual process involving content analysis of student writings. With recent advancements in the field of learning analytics, there have been several attempts to use text analytics to examine student reflective writings. This paper presents the results of a study examining the use of theoretically-sound linguistic indicators of different psychological processes for the development of an analytics system for assessment of reflective writing. More precisely, we developed a random-forest classification system using linguistic indicators provided by the LIWC and Coh-Metrix tools. We also examined what particular indicators are representative of the different types of student reflective writings.", "authors": ["Vitomir Kovanovi\u0107", "Sre\u0107ko Joksimovi\u0107", "Negin Mirriahi", "Ellen Blaine", "Dragan Ga\u0161evi\u0107", "George Siemens", "Shane Dawson"]}, {"title": "What exactly do students learn when they practice equation solving?: refining knowledge components with the additive factors model", "pages": "399-408", "doi": "10.1145/3170358.3170411", "abstract": "Accurately modeling individual students' knowledge growth is important in many applications of learning analytics. A key step is to decompose the knowledge targeted in the instruction into detailed knowledge components (KCs). We search for an accurate KC model for basic equation solving skills, using data from an intelligent tutoring system (ITS), Lynnette. Key criteria are data fit and predictive accuracy based on a standard logistic model called the Additive Factors Model (AFM). We focus on three difficulty factors for equation solving: understanding of variables, the negative sign, and the complexity of the equation. Fine-grained KC models were found to have greater fit and predictive accuracy than an \"ideal,\" more abstract model, indicating that there is substantial under-generalization in students' equation-solving skill related to all three difficulty factors. The work enhances scientific understanding of the challenges students face in learning equation solving. It illustrates how learning analytics could inform the improvement of technology-enhanced learning environments.", "authors": ["Yanjin Long", "Kenneth Holstein", "Vincent Aleven"]}, {"title": "Can't get more satisfaction?: game-theoretic group-recommendation of educational resources", "pages": "409-416", "doi": "10.1145/3170358.3170371", "abstract": "Students' satisfaction from educational resources is a subjective perception of how well these resources meet students' expectations for learning. Recommending educational resources to groups of students, targeting at optimizing all students' satisfaction, is a complicated task due to the lack of joint group profiles. Instead of merging individual profiles or fusing individual recommendations, this paper follows a game-theoretic perspective for solving conflict of interest among students and recommending resources to groups in online collaborative learning contexts: the group members are the players, the resources comprise the set of possible actions, and maximizing each individual member's satisfaction from the selected resources is a problem of finding the Nash Equilibrium. In case the Nash Equilibrium is Pareto efficient, none of the players can get more payoff (satisfaction) without decreasing the payoff of any other player, indicating an optimal benefit for the group as a whole. The comparative evaluation of the suggested approach to other state-of-the-art methods provided statistically significant results regarding the error in predicted group satisfaction from the recommendation and the goodness of the ranked list of recommendations.", "authors": ["Zacharoula Papamitsiou", "Anastasios A. Economides"]}, {"title": "The teacher in the loop: customizing multimodal learning analytics for blended learning", "pages": "417-426", "doi": "10.1145/3170358.3170364", "abstract": "In blended learning scenarios, evidence needs to be gathered from digital and physical spaces to obtain a more complete view of the teaching and learning processes. However, these scenarios are highly heterogeneous, and the varying data sources available in each particular context can condition the accuracy, relevance, interpretability and actionability of the Learning Analytics (LA) solutions, affecting also the user's sense of agency and trust in such solutions. To aid stakeholders in making use of learning analytics, we propose a process to involve teachers in customizing multimodal LA (MMLA) solutions, adapting them to their particular blended learning situation (e.g., identifying relevant data sources and metrics). Since measuring the added value of adopting an LA solution is not straightforward, we also propose a concrete method for doing so. The results obtained from two case studies in authentic, blended computer-supported collaborative learning settings show an improvement in the sensitivity and F1 scores of the customized MMLA solution. Aside from these quantitative improvements, participant teachers reported both an increment in the effort involved, but also increased relevance, understanding and actionability of the results.", "authors": ["Mar\u00eda Jes\u00fcs Rodr\u00edguez-Triana", "Luis P. Prieto", "Alejandra Mart\u00ednez-Mon\u00e9s", "Juan I. Asensio-P\u00e9rez", "Yannis Dimitriadis"]}, {"title": "Analytics-enabled teaching as design: reconceptualisation and call for research", "pages": "427-435", "doi": "10.1145/3170358.3170390", "abstract": "As a human-centred educational practice and field of research, learning analytics must account for key stakeholders in teaching and learning. The focus of this paper is on the role of institutions to support teachers to incorporate learning analytics into their practice by understanding the confluence of internal and external factors that influence what they do. In this paper, we reconceptualise `teaching as design' for `analytics-enabled teaching as design' to shape this discussion to allow for the consideration of external factors, such as professional learning or ethical considerations of student data, as well as personal considerations, such as data literacy and teacher beliefs and identities. In order to address the real-world challenges of progressing teachers' efficacy and capacity toward analytics-enabled teaching as design, we have placed the teacher - as a cognitive, social, and emotional being - at the center. In so doing, we discuss potential directions towards research for practice in elucidating underpinning factors of teacher inquiry in the process of authentic design.", "authors": ["Sakinah S. J. Alhadad", "Kate Thompson", "Simon Knight", "Melinda Lewis", "Jason M. Lodge"]}, {"title": "The complexities of developing a personal code of ethics for learning analytics practitioners: implications for institutions and the field", "pages": "436-440", "doi": "10.1145/3170358.3170396", "abstract": "In this paper we explore the potential role, value and utility of a personal code of ethics (COE) for learning analytics practitioners, and in particular we consider whether such a COE might usefully mediate individual actions and choices in relation to a more abstract institutional COE. While several institutional COEs now exist, little attention has been paid to detailing the ethical responsibilities of individual practitioners. To investigate the problems associated with developing and implementing a personal COE, we drafted an LA Practitioner COE based on other professional codes, and invited feedback from a range of learning analytics stakeholders and practitioners: ethicists, students, researchers and technology executives. Three main themes emerged from their reflections: 1. A need to balance real world demands with abstract principles, 2. The limits to individual accountability within the learning analytics space, and 3. The continuing value of debate around an aspirational code of ethics within the field of learning analytics.", "authors": ["Charles Lang", "Leah P. Macfadyen", "Sharon Slade", "Paul Prinsloo", "Niall Sclater"]}, {"title": "Online change detection for monitoring individual student behavior via clickstream data on E-book system", "pages": "446-450", "doi": "10.1145/3170358.3170412", "abstract": "We propose a new change detection method using clickstream data collected through an e-Book system. Most of the prior work has focused on the batch processing of clickstream data. In contrast, the proposed method is designed for online processing, with the model parameters for change detection updated sequentially based on observations of new click events. More specifically, our method generates a model for an individual student and performs minute-by-minute change detection based on click events during a classroom lecture. We collected clickstream data from four face-to-face lectures, and conducted experiments to demonstrate how the proposed method discovered change points and how such change points correlated with the students' performances.", "authors": ["Atsushi Shimada", "Yuta Taniguchi", "Fumiya Okubo", "Shin'ichi Konomi", "Hiroaki Ogata"]}, {"title": "Embracing imperfection in learning analytics", "pages": "451-460", "doi": "10.1145/3170358.3170413", "abstract": "Learning Analytics (LA) sits at the confluence of many contributing disciplines, which brings the risk of hidden assumptions inherited from those fields. Here, we consider a hidden assumption derived from computer science, namely, that improving computational accuracy in classification is always a worthy goal. We demonstrate that this assumption is unlikely to hold in some important educational contexts, and argue that embracing computational \"imperfection\" can improve outcomes for those scenarios. Specifically, we show that learner-facing approaches aimed at \"learning how to learn\" require more holistic validation strategies. We consider what information must be provided in order to reasonably evaluate algorithmic tools in LA, to facilitate transparency and realistic performance comparisons.", "authors": ["Kirsty Kitto", "Simon Buckingham Shum", "Andrew Gibson"]}, {"title": "The pragmatic maxim as learning analytics research method", "pages": "461-465", "doi": "10.1145/3170358.3170384", "abstract": "It is arguable that the chief aim of Learning Analytics is to use analytics for meaningful purposes in learning and teaching contexts, and that research in the field should advance this cause. However the field does not present a single clear understanding of what constitutes quality in Learning Analytics research. In this paper we present the Pragmatic Inquiry for Learning Analytics Research (PILAR) method as one approach to conducting Learning Analytics research. Rather than creating a new method, we reintroduce an old method to a new field, drawing on the Pragmatic Maxim, proposed by Charles Sanders Peirce as a principle for making ideas clear. Our instantiation of the Pragmatic Maxim requires the researcher to situate Learning Analytics research within a clearly defined learning context and to consider the analytics in terms of the practical effects on learning. We propose three essential elements and a five step process for addressing them in research. After presenting PILAR we address two potential limitations of the approach, and conclude with some implications for its future use in Learning Analytics research.", "authors": ["Andrew Gibson", "Charles Lang"]}, {"title": "Methodological foundations for the measurement of learning in learning analytics", "pages": "466-470", "doi": "10.1145/3170358.3170391", "abstract": "Learning analysts often claim to measure learning, but their work has attracted growing concern about whether or not the measures are sufficiently accurate, fair, reliable, and valid, with utility for educators and interpretable by them. This paper considers these issues in the light of practices of scholars in more established fields, educational measurement particularly. The focus is on what really matters about methodologies for measuring learning, including foundational assumptions about the nature of learning, what is understood by the term `measured', the criteria applied when assessing quality of data, the standards of proof required to establish validity, reliability, generalizability, utility and interpretability of findings, and assumptions about learners and learning underlying data modeling techniques used to abstract meaning from the data. This paper argues that, for learning analytics to take its place as a fully-fledged member of the learning sciences, it needs seriously to consider how to measure learning. Methodology crafted at the interface of measurement science and learning analytics may be of sufficient interest to create a new subfield of scholarship - dubbed here `metrilytics' - to make a distinctive contribution to the science of learning.", "authors": ["Sandra K. Milligan"]}]